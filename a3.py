# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17PN_2wm_0H69HL_mAy9PkcrjXxjIQXRQ
"""

import sklearn.decomposition as decom
import pickle
import numpy as np
import matplotlib.pyplot as plt
import math
import sklearn.discriminant_analysis as dis
from sklearn.utils import resample
from sklearn.utils.testing import ignore_warnings
from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.mixture import GaussianMixture


with open("mnistTVT.pickle","rb") as f: 
  Xtrain,Ttrain,Xval,Tval,Xtest,Ttest = pickle.load(f)
Xtrain = Xtrain.astype(np.float64) 
Xval = Xval.astype(np.float64) 
Xtest = Xtest.astype(np.float64)

print("Question 1(a)") 
print("-------------")

def get_xp(X1, X2, D):
  clf = decom.PCA(D)
  clf.fit(X1)
  reduced = clf.transform(X2)
  projected = clf.inverse_transform(reduced)
  return projected

def plot_first(projected):
  sample = projected[0:26]
  plt.figure()
  for i in range(len(sample) -1):
    plt.subplot(5, 5, i+1)
    plt.axis('off')
    plt.imshow(sample[i].reshape(28,28))

X_p = get_xp(Xtrain, Xtest, 30)
plot_first(X_p)
plt.suptitle("Question 1(a): MNIST test data projected onto 30 dimensions")
plt.show()

print("Question 1(b)") 
print("-------------")
X_p = get_xp(Xtrain, Xtest, 3)
plot_first(X_p)
plt.suptitle("Question 1(b): MNIST test data projected onto 3 dimensions")
plt.show()

print("Question 1(c)") 
print("-------------")
X_p = get_xp(Xtrain, Xtest, 300)
plot_first(X_p)
plt.suptitle("Question 1(c): MNIST test data projected onto 300 dimensions")
plt.show()

print("Question 1(d)") 
print("-------------")
def myPCA(X,K):
  mu = np.mean(X)
  cov = np.cov(X)
  eigenvalues, eigenvectors= np.linalg.eigh(cov)
  eigenvectors = np.array(eigenvectors[-K:])
  z = np.matmul(eigenvectors, (X - mu))

  #x_p = mu + np.sum(np.matmul(eigenvectors.T, z))
  #x_p = mu + sum(np.matmul(z, eigenvectors).T)
  x_p=  mu + np.matmul(eigenvectors.T, z)
  print("xp")
  print(x_p)
  return x_p


print("Question 1(f)") 
print("-------------")
def rms(X1, X2):
  X = X1-X2
  inner = np.sum(np.square(X)/len(X))
  return np.sqrt(inner)

Xtrain = Xtrain[:5000]
myXtrainP = myPCA(Xtrain, 100)
plot_first(myXtrainP)
plt.suptitle("Question 1(f): MNIST data projected onto 100 dimensions (mine)")
plt.show()

clf = decom.PCA(100, svd_solver="full")
clf.fit(Xtrain)
reduced = clf.transform(Xtrain)
XtrainP = clf.inverse_transform(reduced)
plot_first(XtrainP)
plt.suptitle("Question 1(f): MNIST data projected onto 100 dimension (sklearn)")
plt.show()
 
print("RMS:", rms(XtrainP, myXtrainP))


print("Question 2(a)") 
print("-------------")
clf = dis.QuadraticDiscriminantAnalysis()
clf.fit(Xtrain[:200], Ttrain[:200])
accuracy_small = clf.score(Xtrain[:200], Ttrain[:200])
accuracy_full = clf.score(Xtest, Ttest) 
print("Training Accuracy:", accuracy_small)
print("Testing Accuracy:", accuracy_full)

print("Question 2(b)") 
print("-------------")
def get_rp(rp, Xtrain, Ttrain):
  max_val_acc = 0
  corr_training_acc = 0
  max_reg_param = -1
  training_acc = []
  validation_acc = []
  
  for i in rp:
    clf = dis.QuadraticDiscriminantAnalysis(reg_param = i)
    clf.fit(Xtrain[:200], Ttrain[:200])
    accuracy_small = clf.score(Xtrain[:200], Ttrain[:200])
    accuracy_full = clf.score(Xval, Tval) 
    training_acc.append(accuracy_small)
    validation_acc.append(accuracy_full)
    if accuracy_full > max_val_acc:
      max_val_acc = accuracy_full
      corr_training_acc = accuracy_small
      max_reg_param = i
  return max_val_acc, corr_training_acc, max_reg_param, training_acc, validation_acc

regularization_params = 1/ 2**(np.array(range(21)))
max_val_acc, corr_training_acc, max_reg_param, training_acc, validation_acc = get_rp(regularization_params, Xtrain, Ttrain)
print("Max Validation Accuracy:", max_val_acc)
print("Corresponding Training Accuracy:", corr_training_acc)
print("Regularization Parameter:", max_reg_param)
plt.semilogx(regularization_params,training_acc, "b")
plt.semilogx(regularization_params,validation_acc, "r")
plt.xlabel("Regularization parameter")
plt.ylabel("Accuracy")
plt.title("Question 2(b): Training and Validation Accuracy for Regularized QDA")
plt.show()


print("Question 2(d)") 
print("-------------")

with open("mnistTVT.pickle","rb") as f: 
  Xtrain,Ttrain,Xval,Tval,Xtest,Ttest = pickle.load(f)
Xtrain = Xtrain.astype(np.float64) 
Xval = Xval.astype(np.float64) 
Xtest = Xtest.astype(np.float64)

def train2d(K,X,T):
  clf = decom.PCA(n_components = K, svd_solver="full")
  ignore_warnings(clf.fit)(X)
  reduced = clf.transform(X)
  clf2 = dis.QuadraticDiscriminantAnalysis()
  ignore_warnings(clf2.fit)(reduced,T)
  accuracy = clf2.score(reduced, T) 
  return clf, clf2, accuracy

def test2d(pca,qda,X,T):
  reduced = pca.transform(X)
  return qda.score(reduced, T)

for k in range(1, 51):
  pca, qda, accuracy = train2d(k, Xtrain[:200], Ttrain[:200])
  accuracy_test = test2d(pca, qda, Xval, Tval)
kparams = range(1,51)
max_val_acc, corr_training_acc, max_reg_param, training_acc, validation_acc = get_rp(kparams, Xtrain, Ttrain)
print("Max Validation Accuracy:", max_val_acc)
print("Corresponding Training Accuracy:", corr_training_acc)
print("K Value:", max_reg_param)
plt.plot(kparams,training_acc, "b")
plt.plot(kparams,validation_acc, "r")
plt.xlabel("Reduced Dimension")
plt.ylabel("Accuracy")
plt.title("Question 2(d): Training and Validation Accuracy for PCA + QDA")
plt.show()


print("Question 2(f)") 
print("-------------")

def combined(K, rp, Xtrain, Ttrain, Xval, Tval):
  accMax = 0
  corr_training_acc = 0
  max_reg_param = -1
  training_acc = []
  validation_acc = []
  max_k = -1
  accMaxK = []
  accMaxKCounter = 0
  currentMaxAcc = 0
  for k in K:
    for r in rp:
      clf = decom.PCA(n_components = k, svd_solver="full")
      ignore_warnings(clf.fit)(Xtrain)
      reduced = clf.transform(Xtrain)
      new_Xval = clf.transform(Xval)
      clf2 = dis.QuadraticDiscriminantAnalysis(reg_param = r)
      ignore_warnings(clf2.fit)(reduced,Ttrain)
      accuracy_small = clf2.score(reduced, Ttrain) 
      accuracy_full = clf2.score(new_Xval, Tval)
      training_acc.append(accuracy_small)
      validation_acc.append(accuracy_full)
      if accuracy_full > accMax:
        accMax = accuracy_full
        corr_training_acc = accuracy_small
        max_reg_param = r
        max_k = k
      if accuracy_full > currentMaxAcc:
        accMaxKCounter = r
        currentMaxAcc = accuracy_full

    accMaxK.append(accMaxKCounter)
    accMaxKCounter = 0
    currentMaxAcc = 0
  return accMaxK, accMax, corr_training_acc, max_reg_param, max_k, training_acc, validation_acc

accMaxK, accMax, corr_training_acc, max_reg_param, max_k, training_acc, validation_acc = combined(kparams, regularization_params, Xtrain[:200], Ttrain[:200], Xval, Tval) 
print("Max Validation Accuracy:", max_val_acc)
print("Corresponding Training Accuracy:", corr_training_acc)
print("Regularization Parameter:", max_reg_param)
print("K Values:", max_k)
plt.plot(kparams,accMaxK)
plt.xlabel("Reduced Dimension")
plt.ylabel("Accuracy")
plt.title("Question 2(f): Maximum validation accuracy for QDA")
plt.show()

print("Question 3(a)") 
print("-------------")
def myBootstrap(X,T):
  while True:
    bootstrap = resample(X, T)
    num_values = np.unique(bootstrap[0], return_counts = True)
    if np.all(num_values[1] > 2):
      break
  return bootstrap

print("Question 3(b)") 
print("-------------")
def bootstrap_accuracy(Xtrain, Ttrain, Xval, Tval, num_samples):
  clf = dis.QuadraticDiscriminantAnalysis(reg_param = 0.004)
  ignore_warnings(clf.fit)(Xtrain, Ttrain)
  validation_accuracy_base = clf.score(Xval, Tval)
  validation_accuracies = [np.zeros((len(Xval), 10))]
  for i in range(num_samples):
    boot_sample = myBootstrap(Xtrain, Ttrain)
    clf = dis.QuadraticDiscriminantAnalysis(reg_param = 0.004)
    ignore_warnings(clf.fit)(boot_sample[0], boot_sample[1])
    validation_accuracies.append(validation_accuracies[-1] + clf.predict_proba(Xval))
  average_val_accuracy = np.array(validation_accuracies[-1] / len(validation_accuracies)-1)
  predictions_vector = np.argmax(average_val_accuracy, axis=1)
  predictions_vector_acc = sum(predictions_vector == Tval)/len(Tval)
  return validation_accuracy_base, predictions_vector_acc, validation_accuracies

base_val_accuracy, bootstrap_val_accuracy, validation_accuracies = bootstrap_accuracy(Xtrain[:200], Ttrain[:200], Xval, Tval, 50)
print("Validation Accuracy Base:", base_val_accuracy)
print("Validation accuracy of the bagged classifier:", bootstrap_val_accuracy)

print("Question 3(c)") 
print("-------------")

base_val_accuracy, bootstrap_val_accuracy, validation_accuracies = bootstrap_accuracy(Xtrain[:200], Ttrain[:200], Xval, Tval, 500)
print("Validation Accuracy Base:", base_val_accuracy)
print("Validation accuracy of the bagged classifier:", bootstrap_val_accuracy)
val_accs = []
for a in validation_accuracies[1:]:
  average_val_accuracy = np.array(a / len(validation_accuracies)-1)
  predictions_vector = np.argmax(average_val_accuracy, axis=1)
  val_accs.append(sum(predictions_vector == Tval)/len(Tval))

plt.plot(range(1,501),val_accs)
plt.xlabel("Number of bootstrap samples")
plt.ylabel("Accuracy")
plt.title("Question 3(c): Validation accuracy")
plt.show()

plt.semilogx(range(1,501),val_accs)
plt.xlabel("Number of bootstrap samples")
plt.ylabel("Accuracy")
plt.title("Question 3(c): Validation accuracy (log scale)")
plt.show()

print("Question 3(d)") 
print("-------------")
def train3d(K,R,X,T):
  clf = decom.PCA(n_components = K, svd_solver="full")
  ignore_warnings(clf.fit)(X)
  reduced = clf.transform(X)
  clf2 = dis.QuadraticDiscriminantAnalysis(reg_param=R)
  ignore_warnings(clf2.fit)(reduced,T) 
  return clf, clf2

def proba3d(pca,qda,X):
  reduced = pca.transform(X)
  prob = qda.predict_proba(reduced)
  return prob

print("Question 3(e)") 
print("-------------")
def myBag(K,R):
  global Xtrain
  global Ttrain
  global Xval
  global Tval
  pca, qda = train3d(K, R, Xtrain[:200], Ttrain[:200])
  reduced = pca.transform(Xval)
  val_acc_base = qda.score(reduced, Tval)
  predicted_prob_val = [np.zeros((len(Xval), 10))]
  for i in range(200):
    boot_sample = myBootstrap(Xtrain[:200], Ttrain[:200])
    pca2, qda2 = train3d(K, R, boot_sample[0], boot_sample[1])
    predicted_prob_val.append(predicted_prob_val[-1] + proba3d(pca2, qda2, Xval))
  average_val_accuracy = np.array(predicted_prob_val[-1] / len(predicted_prob_val)-1)
  predictions_vector = np.argmax(average_val_accuracy, axis=1)
  predictions_vector_acc = np.sum(predictions_vector == Tval)/len(Tval)
  return val_acc_base, predictions_vector_acc

print("Question 3(f)") 
print("-------------")
base, bagged = myBag(100,0.01)
print("Validation Accuracy Base:", base)
print("Validation accuracy of the bagged classifier:", bagged)

print("Question 3(g)") 
print("-------------")
base_accs = []
bagged_accs = []
for i in range(50):
  K = np.random.randint(1, 10)
  R = np.random.uniform(0.2, 1.0)
  base, bagged = myBag(K,R)
  base_accs.append(base)
  bagged_accs.append(bagged)

plt.plot(base_accs, bagged_accs, "bo")
plt.plot([0,1], [0,1], "r")
plt.xlabel("Base validation accuracy")
plt.ylabel("Bagged validation accuracy")
plt.title("Question 3(g): Bagged v.s. base validation accuracy")
plt.show()

print("Question 3(h)") 
print("-------------")
base_accs = []
bagged_accs = []
for i in range(50):
  K = np.random.randint(50, 200)
  R = np.random.uniform(0.0, 0.05)
  base, bagged = myBag(K,R)
  base_accs.append(base)
  bagged_accs.append(bagged)

print("Maximum bagged validation accuracy:", max(bagged_accs))
plt.plot(base_accs, bagged_accs, "bo")
plt.plot([0,1],[max(bagged_accs), max(bagged_accs)], "r")
plt.ylim(0,1)
plt.xlabel("Base validation accuracy")
plt.ylabel("Bagged validation accuracy")
plt.title("Question 3(h): Bagged v.s. base validation accuracy")
plt.show()

print("Question 4(a)") 
print("-------------")
with open("dataA2Q2.pickle","rb") as file: 
  dataTrain,dataTest = pickle.load(file)
Xtrain,Ttrain = dataTrain
Xtest,Ttest = dataTest
def plot_clusters(X,R,Mu):
  #first add up the total responsibility in each column of R
  R_sum = np.sum(R,axis=0)
  #Then sort the columns of R in ascending order, so the column with least total responsibility is first
  i = np.argsort(R_sum) #i is the indices of sorted R 
  new_R = R.T[i].T
  plt.scatter(X[:, 0], X[:, 1], color=new_R, s=5)
  plt.scatter(Mu.T[0], Mu.T[1], color="black")
  
print("Question 4(b)") 
print("-------------")

kmeans = KMeans(n_clusters=3).fit(Xtrain)
Mu = kmeans.cluster_centers_
#one-hot encoding of R
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(kmeans.labels_)
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
R = onehot_encoder.fit_transform(integer_encoded)

print("K-means objective function (Train): ", kmeans.score(Xtrain))
print("K-means objective function (Test): ", kmeans.score(Xtest))

plot_clusters(Xtrain, R, Mu)
plt.title("Question 4(b): K means")
plt.show()

print("Question 4(c)") 
print("-------------")
GM = GaussianMixture(n_components = 3, covariance_type="spherical").fit(Xtrain)
Mu = GM.means_
R = GM.predict_proba(Xtrain)
print("Gaussian Mixture objective function (Train & Sperical type): ", GM.score(Xtrain))
print("Gaussian Mixture objective function (Test & Sperical type): ", GM.score(Xtest))

plot_clusters(Xtrain, R, Mu)
plt.title("Question 4(c): Gaussian mixture model (spherical)")
plt.show()

print("Question 4(d)") 
print("-------------")
GM2 = GaussianMixture(n_components = 3, covariance_type="full").fit(Xtrain)
Mu = GM2.means_
R = GM2.predict_proba(Xtrain)
print("Gaussian Mixture objective function (Train & Full type): ", GM2.score(Xtrain))
print("Gaussian Mixture objective function (Test & Full type): ", GM2.score(Xtest))

plot_clusters(Xtrain, R, Mu)
plt.title("Question 4(d): Gaussian mixture model (full)")
plt.show()

print("Q4d-Q4c test scores =", GM2.score(Xtest) - GM.score(Xtest))

print("Question 4(e)") 
print("-------------")
def myKmeans(X,K,I):
  #Initilize cluster centres to K random points in X
  m = X[np.random.randint(0, high=len(X),size=K)]
  R = [] #has shape (I,len(X),K)
  Score = []
  for i in range(I):
    r = []
    s = []
    for x in X:
      j = (m-x)
      j = np.sum(j, axis=1)**2
      k = np.argmin(j)
      s.append(j)
      temp = np.zeros(K)
      temp[k] = 1
      r.append(temp)
    r = np.array(r)
    R.append(r)
    Score.append(np.sum(np.array(s)))
    m = np.matmul(r.T, X)/ np.array([np.sum(r, axis=0), np.sum(r, axis=0)]).T#np.sum(np.matmul(r.T, X))/np.sum(r,axis = 1)

  R_sum = np.sum(R[-1],axis=0)
  #Then sort the columns of R in ascending order, so the column with least total responsibility is first
  i = np.argsort(R_sum) #i is the indices of sorted R 
  new_R = R[-1].T[i].T

  return m, new_R, Score
  #return m, R[-1], Score

def scoreKmeans(X,Mu):
  pass
  #I do not know.

m, R, Score = myKmeans(Xtrain, 3, 100)
plt.plot(range(1,21), Score[:20])
plt.title("Question 4(e): score v.s. iteration (K means)")
plt.xlabel("Iteration")
plt.ylabel("Score")
plt.show()

plot_clusters(Xtrain, np.array(R), m)
plt.title("Question 4(e): Data clustered by K means")
plt.show()