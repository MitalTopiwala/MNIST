# -*- coding: utf-8 -*-
"""CSC311A1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v8TpndvSFbQaXcF4vkWxS0oS46gyiwTr
"""

import numpy as np
import numpy.random as rnd
import time
import math
import bonnerlib3D as bl3d
import matplotlib.pyplot as plt
import pickle
import sklearn.neighbors

rnd.seed(3)

print("Question 1") 
print("----------")

print("Question 1(a):") 
B = rnd.rand(4,5)
print(B)

print("Question 1(b):") 
y = rnd.rand(4,1)
print(y)

print("Question 1(c):") 
C = B.reshape((2, 10))
print(C)

print("Question 1(d):")
D = B - y
print(D)

print("Question 1(e):")
z = y.reshape(4)
print(z)

print("Question 1(f):")
B[:,3] = z
print(B)

print("Question 1(g):")
D[:,0] = B[:,2] + z
print(D)

print("Question 1(h):")
print(B[:3])

print("Question 1(i):") 
print(np.array([B[:,1], B[:,3]]))

print("Question 1(j):") 
print(np.log(B))

print("Question 1(k):")
print(np.sum(B))

print("Question 1(l):")
print(B.max(axis = 0))

print("Question 1(m):")
print(max(B.sum(axis=1)))

print("Question 1(n):") 
print(np.matmul(B.T, D))

print("Question 1(o):")#incomplete
print(np.matmul(np.matmul(np.matmul(y.T, D), D.T), y))

print("Question 2") 
print("----------")

#print("Question 2(a):")

def matrix_poly(A):
    for o in range(2):
        temp = [np.zeros((len(A), len(A))), np.zeros((len(A), len(A)))]
        for i in range(len(A)):
            for j in range(len(A)):
                if o == 0:
                    temp[o][i][j] = A[i][j]*A[j][i]
                else:
                    temp[o][i][j] = A[i][j]*(A[j][i] + temp[0][j][i])

    return A+temp[1]

#print("Question 2(b):")

def timing(N):
    A = rnd.rand(N,N)
    start_time1 = time.time()
    B1 = matrix_poly(A) 
    print("matrix_poly takes %s seconds" % (time.time() - start_time1))
    start_time2 = time.time()
    B2 = A + (np.matmul(A, A + np.matmul(A, A))) 
    print("Vectorized takes %s seconds" % (time.time() - start_time2))
    mag_diff = math.sqrt(np.sum((B1-B2)**2))
    print(mag_diff)


print("Question 2(c):")

print("timing(100):")
timing(100)
print("timing(300):")
timing(300)
print("timing(1000):")
timing(1000)
#Q: In each case, how many floating-point multiplications does matrix_poly perform?
# matrix_poly performs 2*n^2 floating point mulitplications

print("Question 3") 
print("----------")
#print("Question 3(a):")

def least_squares(x,t): 
    X = np.concatenate((np.ones((len(x), 1)), x.reshape((len(x), 1))), axis=1)
    w = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T), t)

    return [w[1], w[0]]#returns optimal [a,b]

#print("Question 3(b):")

def plot_data(x,t):
    para = least_squares(x,t)
    plt.suptitle("Question 3(b): the fitted line")
    plt.scatter(x, t) # create scatter plot of training data
    y = para[0]*x + para[1]
    plt.plot(x, y) # create fitted line

#print("Question 3(c):")

def error(a,b,X,T): #?? Double check
    y = a*X+b
    mse = sum((T-y)**2) / len(X) 
    return mse

print("Question 3(d):")

with open("dataA1Q3.pickle","rb") as f: 
  dataTrain,dataTest = pickle.load(f)


plot_data(dataTrain[0], dataTrain[1])
coef = least_squares(dataTrain[0], dataTrain[1])
print("a is %s and b is %s" % (coef[0], coef[1]))

print("Training error")
print(error(coef[0], coef[1], dataTrain[0], dataTrain[1]))
print("Test error")
print(error(coef[0], coef[1], dataTest[0], dataTest[1]))


print("Question 4") 
print("----------")
print("Question 4(a):")

import sklearn.linear_model as lin
with open("dataA1Q4v2.pickle","rb") as f: 
    Xtrain,Ttrain,Xtest,Ttest = pickle.load(f)
clf = lin.LogisticRegression() # create a classification object, clf clf.fit(Xtrain,Ttrain) # learn a logistic-regression classifier
clf.fit(Xtrain,Ttrain)
w = clf.coef_[0] # weight vector
w0 = clf.intercept_[0] # bias term
print("Weight vector: %s" % (w))
print("Bias Term: %s" % (w0))


print("Question 4(b):")

#using score
clf.score(Xtrain, Ttrain)
accuracy1 = clf.score(Xtrain, Ttrain)  #average number of correct predictions
#using w and w0
#print(w)
#print(Xtest)
z = np.matmul(Xtest, w) + w0
y = np.where(z>=0, 1, 0)
accuracy2 = np.sum(y == Ttest)/len(y) #FIX??
print("accuracy1: %s" % (accuracy1))
print("accuracy2: %s" % (accuracy2))
print("Difference:", (accuracy1 - accuracy2))

#print("Question 4(c):")
bl3d.plot_db(Xtrain,Ttrain,w,w0,30,5)
plt.suptitle("4(c): Training data and decision boundary")

#print("Question 4(d):")
bl3d.plot_db(Xtrain,Ttrain,w,w0,30,20)
plt.suptitle("4(d): Training data and decision boundary")


print("Question 5:")
print("----------")

def gd_logreg(lrate):
    global Xtrain
    global Xtest
    global Ttrain
    global Ttest
    #print(Xtrain)
    #print(Xtest)
    #print(Ttrain)
    #print(Ttest)
    #Cross Entropy function (loss) :t*np.logaddexp(0,-z) + (1-t)*np.logaddexp(0,z)
    rnd.seed(3)
    N = Xtrain.shape[1]
    weights = rnd.rand(N+1) / 1000 #weights + bias term
    #prepend column of 1s
    Xtrain = np.concatenate((np.ones((len(Xtrain), 1)), Xtrain), axis=1)
    Xtest = np.concatenate((np.ones((len(Xtest), 1)), Xtest), axis=1)
    
    ce_train = []
    ce_test = []
    a_train = []
    a_test = []

    iterations = 0 #counts # iter
  
    #initial weight update
    
    z = np.matmul(weights, Xtrain.T)
    y = 1/(1 + np.exp(-z))
    z2 = np.matmul(weights, Xtest.T)
    y2 = 1/(1 + np.exp(-z2))
    
    weights = weights - ((lrate/N)*np.matmul(Xtrain.T,(y-Ttrain)))
    #print(weights)
    
    go = True
    while (go and iterations < 1000): #limited the iteration because code does not self terminate
      weights = weights - ((lrate/N)*np.matmul(Xtrain.T,(y-Ttrain)))
      #training loss 
      ce_train.append(np.mean(Ttrain*np.logaddexp(0,-z) + (1-Ttrain)*np.logaddexp(0,z)))
      #training accuracy
      holder = y > 0.5
      predictions = holder.astype(int)
      holder2 = predictions == Ttrain
      a_train.append(np.mean(holder2))
      z = np.matmul(weights, Xtrain.T)
      y = 1/(1 + np.exp(-z))
      

      #test loss
      ce_test.append(np.mean(Ttest*np.logaddexp(0,-z2) + (1-Ttest)*np.logaddexp(0,z2)))
      #test accuracy
      holder = y2 > 0.5
      predictions = holder.astype(int)
      holder2 = predictions == Ttest
      a_test.append(np.mean(holder2))
      z2 = np.matmul(weights, Xtest.T)
      y2 = 1/(1 + np.exp(-z2))
      
      iterations +=1
      if (iterations >= 2):
        if (abs(ce_train[-2] - ce_train[-1]) <= 10**(-10)): #part d
          go = False
      
      
    #e)
    print("Weight Vector:", weights)
    print("%s iterations performed" % (iterations))
    print("Learning Rate:", lrate)
    print("Q4 Weight Vector:", w)
    print("Q4 Bias Term:", w0)
    

    #f)
    plt.figure()
    plt.suptitle("Question 5: Training and test loss v.s. iterations")
    plt.plot(ce_test, 'r')
    plt.plot(ce_train, 'b')
    plt.xlabel('Iteration Number')
    plt.ylabel('Cross Entropy')
    plt.show()

    #g)
    plt.figure()
    plt.suptitle("Question 5: Training and test loss v.s. iterations")
    plt.semilogx(ce_test, 'r')
    plt.semilogx(ce_train, 'b')
    plt.xlabel('Iteration Number')
    plt.ylabel('Cross Entropy')
    plt.show()
    #h)
    plt.figure()
    plt.suptitle("Question 5: Training and test accuracy v.s. iterations (log scale)")
    plt.semilogx(a_test, 'r')
    plt.semilogx(a_train, 'b')
    plt.xlabel('Iteration Number')
    plt.ylabel('Accuracy')
    plt.show()

gd_logreg(1)

#NOTE: 5 i) j) k) I donâ€™t know


print("Question 6:")
print("----------") 

#print("Question 6(a):")
def get_index_nums(target, nums, input):
  arr = np.logical_or(target == nums[0], target == nums[1])
  return input[arr]

with open("mnistTVT.pickle","rb") as f: 
  Xtrain,Ttrain,Xval,Tval,Xtest,Ttest = pickle.load(f)

def make_reduced(nums):

  r_Xtrain = get_index_nums(Ttrain, nums, Xtrain)
  r_Ttrain = get_index_nums(Ttrain, nums, Ttrain)
  r_Xval = get_index_nums(Tval, nums, Xval)
  r_Tval = get_index_nums(Tval, nums, Tval)
  r_Xtest = get_index_nums(Ttest, nums, Xtest)
  r_Ttest = get_index_nums(Ttest, nums, Ttest)
  
  small_Xtrain = r_Xtrain[:2001]
  small_Ttrain = r_Ttrain[:2001]

  return r_Xtrain, r_Ttrain, r_Xval, r_Tval, r_Xtest, r_Ttest, small_Xtrain, small_Ttrain

r_Xtrain, r_Ttrain, r_Xval, r_Tval, r_Xtest, r_Ttest, small_Xtrain, small_Ttrain = make_reduced([5,6])

#print("Question 6(b):")
def make_plot(r_Xtrain, title):
  
  sample = r_Xtrain[:16]
  plt.figure()
  plt.suptitle(title)
  for i in range(len(sample)):
    plt.subplot(4, 4, i + 1)
    plt.axis('off')
    plt.imshow(sample[i].reshape(28,28))
  plt.show()

make_plot(r_Xtrain, "Question 6(b): 16 MNIST training images")

print("Question 6(c):")
def knn(k, X, T, r_Xtrain, r_Ttrain):
  knn = sklearn.neighbors.KNeighborsClassifier(k)
  knn.fit(r_Xtrain, r_Ttrain)
  accuracy = knn.score(X, T)
  return accuracy 

def find_best_k(r_Xval, r_Tval, small_Xtrain, small_Ttrain,r_Xtrain, r_Ttrain, title):
  ks = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]
  rtraining_a = []
  validation_a = []
  for i in ks:
    validation_a.append(knn(i, r_Xval, r_Tval, r_Xtrain, r_Ttrain))
    rtraining_a.append(knn(i, small_Xtrain, small_Ttrain, r_Xtrain, r_Ttrain))
  plt.figure()
  plt.suptitle(title)
  plt.plot(ks, validation_a, 'r')
  plt.plot(ks, rtraining_a, 'b')
  plt.xlabel('Number of Neighbours')
  plt.ylabel('Accuracy')
  plt.show()

  best_acc = max(validation_a) #if there are two max's this will return the smaller k
  best_k = ks[validation_a.index(best_acc)]
  print("K = %s produces the greatest validation accuracy: %s" % (best_k, best_acc))
  print("K = %s produces a reduced test accuracy of: %s" % (best_k, best_acc))
  print("Best K = ", best_k)

find_best_k(r_Xval, r_Tval, small_Xtrain, small_Ttrain, r_Xtrain, r_Ttrain,"Question 6(c): Training and Validation Accuracy for KNN, digits 5 and 6")

print("Question 6(d):")
r_Xtrain2, r_Ttrain2, r_Xval2, r_Tval2, r_Xtest2, r_Ttest2, small_Xtrain2, small_Ttrain2 = make_reduced([4,7])
make_plot(r_Xtrain2, "Question 6(d): 16 MNIST training images")
find_best_k(r_Xval2, r_Tval2, small_Xtrain2, small_Ttrain2,r_Xtrain2, r_Ttrain2, "Question 6(d): Training and Validation Accuracy for KNN, digits 4 and 7")
