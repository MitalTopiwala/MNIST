# -*- coding: utf-8 -*-
"""CSC311A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NfAUB0H5LWzKmXAVsWQm0ossMg6Ukn4O
"""

import pickle
import math 
import statistics as s
import matplotlib.pyplot as plt
import numpy as np
import bonnerlib2D as bl2d
import sklearn.neighbors
import sklearn.linear_model as lin
import sklearn.discriminant_analysis as dis
import sklearn.naive_bayes as nb
from scipy.stats import multivariate_normal
from sklearn.neural_network import MLPClassifier

with open("dataA2Q1.pickle","rb") as file: 
  dataTrain,dataTest = pickle.load(file)

print("Question 1(a).") 
print("-------------")

def fit_plot(dataTrain,dataTest,K):
  xs_train = dataTrain[0]
  ts_train = dataTrain[1]
  xs_test = dataTest[0]
  ts_test = dataTest[1]
  plt.figure()
  if not isinstance(K, list):
    K = [K]
  for i in range(len(K)):
    ks = np.arange(1,K[i]+1)
  
  #Calculating z
    new_xs_train = np.array(list(map(lambda x: ks*x, xs_train)))
    s_train = np.sin(new_xs_train)#np.array(list(map(lambda x: np.sin(x), new_xs_train)))
    c_train = np.cos(new_xs_train)#np.array(list(map(lambda x: np.cos(x), new_xs_train)))
    z_train = np.concatenate((np.ones((len(xs_train), 1), dtype=int), s_train, c_train), axis=1)

    new_xs_test = np.array(list(map(lambda x: ks*x, xs_test)))
    s_test = np.sin(new_xs_test)#np.array(list(map(lambda x: np.sin(x), new_xs_test)))
    c_test = np.cos(new_xs_test)#(list(map(lambda x: np.cos(x), new_xs_test)))
    z_test = np.concatenate((np.ones((len(xs_test), 1), dtype=int), s_test, c_test), axis=1)

  #Calculating least sqaure, weight and y's
    least_sqaures = np.linalg.lstsq(z_train,ts_train, rcond=None)
    w = least_sqaures[0]
    y_train = np.matmul(w.T, z_train.T)
    y_test = np.matmul(w.T, z_test.T)

  #Calculating error
    training_error = sum((ts_train-y_train)**2) / len(xs_train)
    testing_error = sum((ts_test-y_test)**2) / len(xs_test)

  #Sort x values
    plt.scatter(xs_train, ts_train, 20)
    #d = dict(zip(xs_train,y_train))
    #xs_train = np.sort(xs_train)
    #y_train = list(d[list(xs_train)])#list(map(lambda x: d[x], xs_train))
    
    ybottom, ytop = plt.ylim()
    xbottom, xtop = plt.xlim()
    plt.ylim(ybottom-5, ytop+5)

    points = np.array([xs_train, y_train])
    points = points.T
    points = points[points[:,0].argsort()]
    xs_train = points.T[0]
    y_train = points.T[1]
    
  #Plotting
    if len(K)>1:
      plt.subplot(4, 3, i+1)
      plt.plot(xs_train, y_train)
    else:
      plt.plot(xs_train, y_train)

  return w, training_error, testing_error

 
fit_plot(dataTrain, dataTest, 4)
plt.suptitle("Question 1(a): the fitted function, k=4")
plt.show()

print("\n")
print("Question 1(b).") 
print("-------------")
w, training_err, testing_err = fit_plot(dataTrain, dataTest, 3)
plt.suptitle("Question 1(b): the fitted function, k=3")
plt.show()
print(3, training_err, testing_err, w)

print("\n")
print("Question 1(c).") 
print("-------------")
w, training_err, testing_err = fit_plot(dataTrain, dataTest, 9)
plt.suptitle("Question 1(c): the fitted function, k=9")
plt.show()
print(9, training_err, testing_err, w)

print("\n")
print("Question 1(d).") 
print("-------------")
w, training_err, testing_err = fit_plot(dataTrain, dataTest, 12)
plt.suptitle("Question 1(d): the fitted function, k=12")
plt.show()
print(12, training_err, testing_err, w)

print("\n")
print("Question 1(e)")
print("-------------")
fit_plot(dataTrain, dataTest, [1,2,3,4,5,6,7,8,9,10,11,12])
plt.suptitle("Question 1(e): the fitted functions for many values of k")
plt.show()

print("\n")
print("Question 1(f)")
print("-------------")
data_t = dataTrain
new_data = np.array_split(data_t, 5, 1)
print(data_t)
print(new_data)
training_errors = []
testing_errors = []
for k in range(1, 13):
  current_training = []
  current_testing = []
  for j in range(len(new_data)):
    training = np.delete(new_data, j, 0)
    c_t = np.concatenate(training, axis=1)
    #print(c_t, new_data[j])
    w, training_err, testing_err = fit_plot(c_t, new_data[j], k)
    current_training.append(training_err)
    current_testing.append(testing_err)
  training_errors.append(s.mean(current_training))
  testing_errors.append(s.mean(current_testing))


training_errors = np.reshape(training_errors, 12)
testing_errors = np.reshape(testing_errors, 12)
plt.figure()
plt.suptitle("Question 1(f): mean training and validation error")
plt.xlabel("K")
plt.ylabel("Mean Error")
y = range(1,13)
plt.semilogy(training_errors, y, "b")
plt.semilogy(testing_errors, y, "r")
plt.show()

#Q2
with open("dataA2Q2.pickle","rb") as file: 
  dataTrain,dataTest = pickle.load(file)
Xtrain,Ttrain = dataTrain
Xtest,Ttest = dataTest

def plot_data(X,T): #assume T is an np array
  red = X[np.where(T == 0)[0]]
  blue = X[np.where(T == 1)[0]]
  green = X[np.where(T == 2)[0]]
  plt.scatter(red.T[0], red.T[1], 2, c="red")
  plt.scatter(blue.T[0], blue.T[1], 2, c="blue")
  plt.scatter(green.T[0], green.T[1], 2, c="green")
   


print("\n")
print("Question 2(a)")
print("-------------")

plot_data(Xtrain, Ttrain)
plt.suptitle("Training Data for Question 2")
plt.show()

print("\n")
print("Question 2(b)")
print("-------------")

def accuracyLR(clf,X,T):
  W = clf.coef_ # weight matrix
  w0 = clf.intercept_ # bias vector
  z = (np.matmul(W, X.T)).T + w0
  e_z = np.exp(z)
  y = e_z / e_z.sum() #softmax function
  max_y = np.argmax(y, axis=1)
  acc = sum(max_y == T) / len(y)
  return acc

# create a classification object
clf = lin.LogisticRegression(multi_class="multinomial", solver="lbfgs")
clf.fit(Xtrain,Ttrain)
#using score
accuracy1 = clf.score(Xtrain, Ttrain) 
#using w and w0
accuracy2 = accuracyLR(clf,Xtrain,Ttrain)
print("accuracy1: %s" % (accuracy1))
print("accuracy2: %s" % (accuracy2))
print("Difference:", (accuracy1 - accuracy2))
#plotting
plot_data(Xtrain, Ttrain)
plt.suptitle("Question 2(b): decision boundaries for logistic regression")
bl2d.boundaries(clf)
plt.show()

print("\n")
print("Question 2(c)")
print("-------------")

def accuracyQDA(clf,X,T):
  means = clf.means_
  cov = clf.covariance_
  prior_prob = clf.priors_
  D = len(X[0])
  y = []
  for i in range(len(cov)):
    y.append(multivariate_normal.pdf(X, mean=means[i], cov=cov[i]) * prior_prob[i]) 
  
  max_y = np.argmax(y, axis=0)
  acc = sum(max_y == T) / len(y[0])
  return acc


clf = dis.QuadraticDiscriminantAnalysis(store_covariance=True) # create a classification object
clf.fit(Xtrain,Ttrain)
#using score
accuracy1 = clf.score(Xtrain, Ttrain)  #average number of correct predictions
#manual computation
accuracy2 = accuracyQDA(clf,Xtrain,Ttrain)

print("accuracy1: %s" % (accuracy1))
print("accuracy2: %s" % (accuracy2))
print("Difference:", (accuracy1 - accuracy2))

#plotting
plot_data(Xtrain, Ttrain)
plt.suptitle("2(c): decision boundaries for quadratic discriminant analysis")
bl2d.boundaries(clf)
plt.show()

print("\n")
print("Question 2(d)")
print("-------------")

def accuracyNB(clf,X,T):
  means = clf.theta_
  var = clf.sigma_
  prior_prob = clf.class_prior_
  X = X.reshape(len(X), 1, 2)
  prob = np.exp(-(((X - means)**2)/(2*var))) / np.sqrt(2*math.pi*var)
  prob = np.prod(prob, axis= 2)
  final = prob*prior_prob
  y = np.argmax(final, axis=1)

  return (sum(y == T) / len(y))

clf = nb.GaussianNB() # create a classification object
clf.fit(Xtrain,Ttrain)
#using score
accuracy1 = clf.score(Xtrain, Ttrain)  #average number of correct predictions
#manual computation
accuracy2 = accuracyNB(clf,Xtrain,Ttrain)

print("accuracy1: %s" % (accuracy1))
print("accuracy2: %s" % (accuracy2))
print("Difference:", (accuracy1 - accuracy2))

#plotting
plot_data(Xtrain, Ttrain)
plt.suptitle("Question 2(d): decision boundaries for Gaussian naive Bayes")
bl2d.boundaries(clf)
plt.show()

print("\n")
print("Question 3(a)")
print("-------------")
#Reload data set 
with open("dataA2Q2.pickle","rb") as file: 
  dataTrain,dataTest = pickle.load(file)
Xtrain,Ttrain = dataTrain
Xtest,Ttest = dataTest

print("\n")
print("Question 3(b)")
print("-------------")
def make_nn(num_units, max_i,seed,X, T, batchsize ="auto"):
  np.random.seed(seed)

  clf2 = MLPClassifier(hidden_layer_sizes = (num_units,), solver='sgd', 
                       learning_rate_init= 0.01,max_iter = max_i, tol = 0.00001,
                       activation="logistic", batch_size= batchsize)
  clf2.fit(X,T)
  accuracy = clf2.score(X, T)  #average number of correct predictions
  print("accuracy: %s" % (accuracy))

  #plotting
  if batchsize == "auto":
    plot_data(X, T)
    bl2d.boundaries(clf2)
  return clf2, accuracy
  

make_nn(1, 1000, 0, Xtrain, Ttrain) #NOTE: 200 is the default for max_iter
plt.suptitle("Question 3(b): Neural net with 1 hidden unit")
plt.show()

print("\n")
print("Question 3(c)")
print("-------------")
make_nn(2, 1000, 0, Xtrain, Ttrain)
plt.suptitle("Question 3(b): Neural net with 2 hidden units")
plt.show()

print("\n")
print("Question 3(d)")
print("-------------")
make_nn(9, 1000, 0, Xtrain, Ttrain)
plt.suptitle("Question 3(b): Neural net with 9 hidden units")
plt.show()

print("\n")
print("Question 3(e)")
print("-------------")

#plt.figure()
for i in range(2,11):
  plt.subplot(3, 3, i-1)
  make_nn(7, 2**i, 0, Xtrain, Ttrain)
plt.suptitle("Question 3(e): different numbers of epochs")
plt.show()

print("\n")
print("Question 3(f)")
print("-------------")
#plt.figure()
for i in range(1,10):
  plt.subplot(3, 3, i)
  make_nn(5, 1000, i, Xtrain, Ttrain)
plt.suptitle("Question 3(f): different initial weights")
plt.show()

print("\n")
print("Question 3(g)")
print("-------------")

def accuracyNN(clf,X,T): #Assume clf has 1 hidden layer
  W = np.array(clf.coefs_)
  w0 = np.array(clf.intercepts_)
  H = X
  H = (np.matmul(W[0].T, H.T)).T + w0[0]
  H = 1/(1 + np.exp(-H))
  H = (np.matmul(W[1].T, H.T)).T + w0[1]
  e_h = np.exp(H)
  H = e_h / e_h.sum()
  y = np.argmax(H, axis=1)
  acc = sum(y == T) / len(y)
  return acc, H # NOTE in this case H rep y from slides

np.random.seed(0)
clf, accuracy1 = make_nn(9, 1000, 0, Xtest, Ttest)
accuracy2, y = accuracyNN(clf,Xtest,Ttest)
print("accuracy1: %s" % (accuracy1))
print("accuracy2: %s" % (accuracy2))
print("Difference:", (accuracy1 - accuracy2))

print("\n")
print("Question 3(g)")
print("-------------")

def ceNN(clf,X,T):
  #accuracy, y = accuracyNN(clf, X, T)
  W = np.array(clf.coefs_)
  w0 = np.array(clf.intercepts_)
  
  H = (np.matmul(W[0].T, X.T)).T + w0[0]
  H = 1/(1 + np.exp(-H))
  H = (np.matmul(W[1].T, H.T)).T + w0[1]
  H = np.max(H, axis=1, keepdims=True)  #added
  e_h = np.exp(H)
  y = e_h / e_h.sum()
  Te = np.reshape(1800, len(y[0]))
  Te = np.zeros((T.size, T.max()+1))
  Te[np.arange(T.size),T] = 1

  CE1 = np.dot(-T.T, (clf.predict_log_proba(X)))

  CE2 = np.dot(-Te.T, np.log(y)) #softmax CE loss function using manually calculated y
  return CE1, CE2

np.random.seed(0)
clf2 = MLPClassifier(hidden_layer_sizes = (9,), solver='sgd', learning_rate_init= 0.01,max_iter = 1000, tol = 0.00001,activation="logistic" )
clf2.fit(Xtest,Ttest)
CE1, CE2 = ceNN(clf2, Xtest, Ttest)
print("CE1: %s" % (CE1))
print(np.shape(CE1))
print("CE2: %s" % (CE2))
print("Difference:", (CE1 - CE2))

print("\n")
print("Question 5(a)")
print("-------------")
with open("mnistTVT.pickle","rb") as f: 
  Xtrain,Ttrain,Xval,Tval,Xtest,Ttest = pickle.load(f)

def get_index_nums(target, nums, input):
  arr = np.logical_or(target == nums[0], target == nums[1])
  return input[arr]

def make_reduced(nums):

  r_Xtrain = get_index_nums(Ttrain, nums, Xtrain)
  r_Ttrain = get_index_nums(Ttrain, nums, Ttrain)
  r_Xtest = get_index_nums(Ttest, nums, Xtest)
  r_Ttest = get_index_nums(Ttest, nums, Ttest)

  r_Ttrain = np.where(r_Ttrain==nums[1], 0, r_Ttrain)
  r_Ttrain = np.where(r_Ttrain==nums[0], 1, r_Ttrain)
  r_Ttest = np.where(r_Ttest==nums[1], 0, r_Ttest)
  r_Ttest = np.where(r_Ttest==nums[0], 1, r_Ttest)
  

  return r_Xtrain, r_Ttrain,  r_Xtest, r_Ttest 

r_Xtrain, r_Ttrain, r_Xtest, r_Ttest = make_reduced([5,6])
r2_Xtrain, r2_Ttrain, r2_Xtest, r2_Ttest = make_reduced([4,5])

print("\n")
print("Question 5(b)")
print("-------------")

def evaluateNN(clf,X,T):
  accuracy1 = clf.score(X,T)
  accuracy2, y = accuracyNN(clf,X,T)
  CE1, CE2 = ceNN(clf, X, T)
  return accuracy1, accuracy2, CE1, CE2